#决策组--强化学习分支


##文件介绍
本项目是用于利用AC算法，根据ICRA2020人工智能挑战赛规则，对双机VS双机进行训练和测试。

agent文件夹中包含了训练模型文件和在训练所用到的手动操作实现以及利用已有的训练模型进行敌机控制的文件

archieved文件夹中包含了可视化界面，以及全局规划，车身的旋转，移动，射击等基本操作设定

battlefield文件夹中定义了机器人本身和障碍物，加成惩罚区的尺寸参数和相关参数设定

imgs文件夹为训练过程中记录的loss和reward值进行记录，以及在测试训练过程中界面的记录

run.sh为训练+测试的快速启动文件

environment.yaml文件包含了整个工程所用到的环境和依赖工具

simulator.py包含主要的操作（eg.平移步长）和参数（eg.奖励惩罚因子）设置

train.py为训练启动函数

test.py为测试启动函数

utils.py为机器人初始化参数设置

ICRA_save.model是已训练出的模型

##软件功能介绍
软件根据真实机器人枪口，炮管，车身尺寸和地图信息进行仿真。
双机对抗过程中我方两车可进行信息交互，互相获取血量，位置，姿态等信息，根据双车信息进行双车路径规划（躲避障碍物和惩罚区）以及决策行动（获取buff，追击敌人，躲避防御）。在敌方车进入我方视野后可以获取到敌方位置，姿态等信息，并可以进行联合追击敌方一车，掩护友方获取buff。

##编译安装方式
本工程主体语言为python3.5，在Ubuntu和Windows平台均可进行安装编译

##原理介绍和理论支持分析
###Actor-Critic
Actor-Critic算法分为两部分，actor的前身是policy gradient，它可以轻松地在连续动作空间内选择合适的动作，value-based的Q-learning只能解决离散动作空间的问题。但是又因为Actor是基于一个episode的return来进行更新的，所以学习效率比较慢。这时候我们发现使用一个value-based的算法作为Critic就可以使用TD方法实现单步更新，这其实可以看做是拿偏差换方差。这样两种算法相互补充就形成了我们的Actor-Critic。Actor来选择动作，Critic来告诉Actor它选择的动作是否合适。在这一过程中，Actor不断迭代，得到每一个状态下选择每一动作的合理概率，Critic也不断迭代，不断完善每个状态下选择每一个动作的奖惩值。
